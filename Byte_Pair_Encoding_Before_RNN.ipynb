{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Byte Pair Encoding Before RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORaJ5RRfLWreSFKXZiOxKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valefe/playground_transformers/blob/main/Byte_Pair_Encoding_Before_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIvsOoLyEfFa",
        "outputId": "785fd5fb-160c-4c8b-ec6d-100230bfd8f4"
      },
      "source": [
        "!pip install bpe\n",
        "!pip install transformers"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bpe in /usr/local/lib/python3.6/dist-packages (1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from bpe) (3.6.4)\n",
            "Requirement already satisfied: mypy in /usr/local/lib/python3.6/dist-packages (from bpe) (0.812)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from bpe) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpe) (4.41.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from bpe) (0.11.1)\n",
            "Requirement already satisfied: hypothesis in /usr/local/lib/python3.6/dist-packages (from bpe) (6.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (20.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (8.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (53.0.0)\n",
            "Requirement already satisfied: mypy-extensions<0.5.0,>=0.4.3 in /usr/local/lib/python3.6/dist-packages (from mypy->bpe) (0.4.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy->bpe) (3.7.4.3)\n",
            "Requirement already satisfied: typed-ast<1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mypy->bpe) (1.4.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from hypothesis->bpe) (2.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRjptthOzy48"
      },
      "source": [
        "from torch import nn, tensor\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdDxF4IfFQgS"
      },
      "source": [
        "df = pd.read_csv(\"description_bisac.csv\", nrows=5000, names=[\"id\", \"mainDescription\", \"bisacCode\"])\n",
        "# TO-DO: Mask some words"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMcjLFiPXcZY"
      },
      "source": [
        "# Tokenizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEhuoI5SR6bk"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWqQCe53XbaS"
      },
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.train([\"./description_bisac.csv\"], trainer=trainer)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUDyIaATVM8b"
      },
      "source": [
        "*\"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqjmk0NFYN0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2b6b44-6db1-48d7-ca7c-4e74f9886717"
      },
      "source": [
        "o_alquimista = \"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"\n",
        "encoded = tokenizer.encode(o_alquimista)\n",
        "encoded.ids[:10]\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2800, 443, 1336, 122, 62, 140, 105, 1993, 11, 860]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZMcCBT0WH6V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lblI7YJ4cTkW"
      },
      "source": [
        "# Creating a Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB217vobcTGo"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbPSYTbdcb98"
      },
      "source": [
        "class MetabooksDataset(Dataset):\n",
        "    \"\"\"Metabooks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, tokenizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            tokenizer (huggingface Tokenizer): Will encode str sentences\n",
        "        \"\"\"\n",
        "        self.genres_df = pd.read_csv(\"description_bisac.csv\", names=[\"id\", \"mainDescription\", \"bisacCode\"])\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.tokenizer.encode(self.genres_df[\"mainDescription\"][idx])\n",
        "        label = self.genres_df[\"bisacCode\"].astype(\"category\").cat.codes[idx]\n",
        "        print(dir(encoded))\n",
        "        print(tensor(encoded.ids))\n",
        "        return {\"text\": tensor(encoded.ids), \"label\": tensor(label)}"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDvUGpAjdKE-"
      },
      "source": [
        "mb_dataset = MetabooksDataset('description_bisac.csv', tokenizer)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hddd52ZgguS",
        "outputId": "0c242b7f-ff08-4a4b-f9d9-7a575664eb42"
      },
      "source": [
        "mb_dataset[0]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'attention_mask', 'char_to_token', 'char_to_word', 'ids', 'merge', 'n_sequences', 'offsets', 'overflowing', 'pad', 'sequence_ids', 'set_sequence_id', 'special_tokens_mask', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'truncate', 'type_ids', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n",
            "tensor([ 607,  212,  103, 3895, 3954, 3983, 3993,  387,  140, 3967, 4008,   83,\n",
            "         371,   13, 2015, 2014,   54, 4015,    9, 2596, 1321, 3982,   95, 2540,\n",
            "        2547,    9, 2545, 1321, 2542,   55,  234, 2558,  141,   70,  745,  113,\n",
            "        1155, 1690, 3986,  103,  231, 4006, 3975,   13, 3965, 4005,  123,   97,\n",
            "         130, 2605,   60,  169, 3973,  134, 2630,   83, 3933, 1701,  103,  700,\n",
            "          70,  392,   56,  297,   11, 2389,  502,  567,   11,  274,  705,  117,\n",
            "        1341,  831,   11,  180, 2541,  106, 2546,  106, 3990,   11,  239,  552,\n",
            "         117,  433, 3935, 2656, 2015, 2014, 3979, 2019,   12,  134,  117,  633,\n",
            "         584, 2465, 3991,  113, 3949,   11, 3969,   12,  134,  117, 2304,   60,\n",
            "        3998,   13, 3994,  113, 3957,   31,   13, 3974, 3992, 2014,   11,  371,\n",
            "          13, 2015, 1722,  552, 2541,  106, 2546,  106, 2619, 3995,   13, 3899,\n",
            "        4013,  117, 4002, 1645,   11,  117, 2542,   11,  117,  140, 1701, 3956,\n",
            "         151, 2545,   11, 3913,   56, 3962,  151, 4009, 4010,  123, 3943,   56,\n",
            "         811,  103, 2540, 2547,   13,  479, 3964, 1722,   11,  130, 2446, 3958,\n",
            "          11, 4004,  140,  148, 2351, 2464,  103,  297,  101, 2416,   13, 1976,\n",
            "         419, 1643, 3963,   56,  268,  103, 3972,  103,  567,  151,  655,   60,\n",
            "         151,  423,   13])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': tensor(0, dtype=torch.int8),\n",
              " 'text': tensor([ 607,  212,  103, 3895, 3954, 3983, 3993,  387,  140, 3967, 4008,   83,\n",
              "          371,   13, 2015, 2014,   54, 4015,    9, 2596, 1321, 3982,   95, 2540,\n",
              "         2547,    9, 2545, 1321, 2542,   55,  234, 2558,  141,   70,  745,  113,\n",
              "         1155, 1690, 3986,  103,  231, 4006, 3975,   13, 3965, 4005,  123,   97,\n",
              "          130, 2605,   60,  169, 3973,  134, 2630,   83, 3933, 1701,  103,  700,\n",
              "           70,  392,   56,  297,   11, 2389,  502,  567,   11,  274,  705,  117,\n",
              "         1341,  831,   11,  180, 2541,  106, 2546,  106, 3990,   11,  239,  552,\n",
              "          117,  433, 3935, 2656, 2015, 2014, 3979, 2019,   12,  134,  117,  633,\n",
              "          584, 2465, 3991,  113, 3949,   11, 3969,   12,  134,  117, 2304,   60,\n",
              "         3998,   13, 3994,  113, 3957,   31,   13, 3974, 3992, 2014,   11,  371,\n",
              "           13, 2015, 1722,  552, 2541,  106, 2546,  106, 2619, 3995,   13, 3899,\n",
              "         4013,  117, 4002, 1645,   11,  117, 2542,   11,  117,  140, 1701, 3956,\n",
              "          151, 2545,   11, 3913,   56, 3962,  151, 4009, 4010,  123, 3943,   56,\n",
              "          811,  103, 2540, 2547,   13,  479, 3964, 1722,   11,  130, 2446, 3958,\n",
              "           11, 4004,  140,  148, 2351, 2464,  103,  297,  101, 2416,   13, 1976,\n",
              "          419, 1643, 3963,   56,  268,  103, 3972,  103,  567,  151,  655,   60,\n",
              "          151,  423,   13])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpBtnyMiPU9"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "SU4LNPyNiSOZ",
        "outputId": "b11017b3-1709-4c89-ad94-f82c72bee75d"
      },
      "source": [
        "for i in range(200):\n",
        "  data = mb_dataset[0]\n",
        "  outputs = model(data[\"text\"], labels=data[\"label\"])\n",
        "  loss = outputs.loss\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-e8e4a1a1f165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmb_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sugfgyriVUxC",
        "outputId": "a527e3db-c4cf-4a6e-95df-99f5a9a0b23c"
      },
      "source": [
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import BertForSequenceClassification  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OUVSCkJWS6y"
      },
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"pt\")"
      ],
      "execution_count": 77,
      "outputs": []
    }
  ]
}