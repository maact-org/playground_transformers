{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Playground - Trying multiple tokenizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valefe/playground_transformers/blob/main/Playground_Trying_multiple_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIvsOoLyEfFa"
      },
      "source": [
        "%%capture magic\n",
        "!pip install bpe\n",
        "!pip install transformers"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRjptthOzy48"
      },
      "source": [
        "from torch import nn, tensor\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdDxF4IfFQgS"
      },
      "source": [
        "df = pd.read_csv(\"description_bisac.csv\", nrows=5000, names=[\"id\", \"mainDescription\", \"bisacCode\"])\n",
        "# TO-DO: Mask some words"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMcjLFiPXcZY"
      },
      "source": [
        "# Tokenizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEhuoI5SR6bk"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWqQCe53XbaS"
      },
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.train([\"./description_bisac.csv\"], trainer=trainer)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUDyIaATVM8b"
      },
      "source": [
        "*\"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqjmk0NFYN0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28c6d75-5891-4bcc-db2b-89aa9ac1cbc9"
      },
      "source": [
        "o_alquimista = \"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"\n",
        "o_alquimista_masked = \"Quando você quer alguma coisa, todo o [MASK] conspira para que você realize seu [MASK]. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"\n",
        "encoded = tokenizer.encode(o_alquimista)\n",
        "print(encoded.ids[:20])\n",
        "encoded = tokenizer.encode(o_alquimista_masked)\n",
        "print(encoded.ids[:20])\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2800, 443, 1336, 122, 62, 140, 105, 1993, 11, 860, 70, 1998, 221, 253, 247, 256, 104, 141, 123, 443]\n",
            "[2800, 443, 1336, 122, 62, 140, 105, 1993, 11, 860, 70, 4, 247, 256, 104, 141, 123, 443, 111, 206]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZMcCBT0WH6V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lblI7YJ4cTkW"
      },
      "source": [
        "# Creating a Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB217vobcTGo"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import argmax"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwxSq7hBRMGv"
      },
      "source": [
        "df = pd.read_csv(\"description_bisac.csv\", names=[\"id\", \"mainDescription\", \"bisacCode\"])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbPSYTbdcb98"
      },
      "source": [
        "class MetabooksDataset(Dataset):\n",
        "    \"\"\"Metabooks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, tokenizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            tokenizer (huggingface Tokenizer): Will encode str sentences\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(\"description_bisac.csv\", names=[\"id\", \"mainDescription\", \"bisacCode\"])\n",
        "        self.size = len(df)\n",
        "        self.tokenizer = tokenizer\n",
        "        self._clean()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame*10)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.tokenizer.encode(self.df[\"mainDescription\"][idx])\n",
        "        encoded_size = len(encoded.ids)\n",
        "        encoded_label = self.df[\"labelOneHot\"][idx]\n",
        "        return {\"text\": tensor(encoded.ids, dtype=torch.int64).view(1, encoded_size), \"label\": tensor(encoded_label, dtype=torch.int64)}\n",
        "\n",
        "    def _clean(self):\n",
        "        tokenizer = self.tokenizer\n",
        "        label_encoder = LabelEncoder()\n",
        "        # One hot encoding for labels\n",
        "        integer_encoded = label_encoder.fit_transform(df[\"bisacCode\"])\n",
        "        print(integer_encoded)\n",
        "        self.df[\"labelOneHot\"] = integer_encoded.tolist()\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDvUGpAjdKE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5433a4a9-ee4d-4004-f192-2f468c32cc8d"
      },
      "source": [
        "mb_dataset = MetabooksDataset('description_bisac.csv', tokenizer)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 4 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hddd52ZgguS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966c6902-1c81-425b-dc54-3d8c3ecaef23"
      },
      "source": [
        "mb_dataset[0]"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': tensor(0),\n",
              " 'text': tensor([[ 607,  212,  103, 3895, 3954, 3983, 3993,  387,  140, 3967, 4008,   83,\n",
              "           371,   13, 2015, 2014,   54, 4015,    9, 2596, 1321, 3982,   95, 2540,\n",
              "          2547,    9, 2545, 1321, 2542,   55,  234, 2558,  141,   70,  745,  113,\n",
              "          1155, 1690, 3986,  103,  231, 4006, 3975,   13, 3965, 4005,  123,   97,\n",
              "           130, 2605,   60,  169, 3973,  134, 2630,   83, 3933, 1701,  103,  700,\n",
              "            70,  392,   56,  297,   11, 2389,  502,  567,   11,  274,  705,  117,\n",
              "          1341,  831,   11,  180, 2541,  106, 2546,  106, 3990,   11,  239,  552,\n",
              "           117,  433, 3935, 2656, 2015, 2014, 3979, 2019,   12,  134,  117,  633,\n",
              "           584, 2465, 3991,  113, 3949,   11, 3969,   12,  134,  117, 2304,   60,\n",
              "          3998,   13, 3994,  113, 3957,   31,   13, 3974, 3992, 2014,   11,  371,\n",
              "            13, 2015, 1722,  552, 2541,  106, 2546,  106, 2619, 3995,   13, 3899,\n",
              "          4013,  117, 4002, 1645,   11,  117, 2542,   11,  117,  140, 1701, 3956,\n",
              "           151, 2545,   11, 3913,   56, 3962,  151, 4009, 4010,  123, 3943,   56,\n",
              "           811,  103, 2540, 2547,   13,  479, 3964, 1722,   11,  130, 2446, 3958,\n",
              "            11, 4004,  140,  148, 2351, 2464,  103,  297,  101, 2416,   13, 1976,\n",
              "           419, 1643, 3963,   56,  268,  103, 3972,  103,  567,  151,  655,   60,\n",
              "           151,  423,   13]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpBtnyMiPU9"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU4LNPyNiSOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd3140d-da44-41c3-b0eb-bb5169b1a342"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKP_Z4C9pEZG",
        "outputId": "181f4cbe-f364-450e-caad-fff5dd99abc8"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(20)):\n",
        "  data = mb_dataset[0]\n",
        "  outputs = model(data[\"text\"], labels=data[\"label\"])\n",
        "  loss = outputs.loss\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:09<00:00,  3.48s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sugfgyriVUxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd4c14d-9032-4961-bae6-568899a8008e"
      },
      "source": [
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import BertForSequenceClassification  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OUVSCkJWS6y"
      },
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"pt\")"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm1u0un6phNt"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9nQSDKepk7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4dc2056-063b-4db5-d19f-0f328d793351"
      },
      "source": [
        "o_alquimista = \"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"\n",
        "encoded = tokenizer.encode(o_alquimista)\n",
        "ids = encoded.ids\n",
        "model(tensor(ids, dtype=torch.int64).view(1, len(ids)))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('logits',\n",
              "                           tensor([[-0.1437,  0.0655]], grad_fn=<AddmmBackward>))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    }
  ]
}