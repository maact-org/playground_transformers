{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Byte Pair Encoding Before RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNCD/mIIgPOXA2l/Wdx5Qvf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valefe/playground_transformers/blob/main/Byte_Pair_Encoding_Before_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIvsOoLyEfFa",
        "outputId": "785fd5fb-160c-4c8b-ec6d-100230bfd8f4"
      },
      "source": [
        "!pip install bpe\n",
        "!pip install transformers"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bpe in /usr/local/lib/python3.6/dist-packages (1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from bpe) (3.6.4)\n",
            "Requirement already satisfied: mypy in /usr/local/lib/python3.6/dist-packages (from bpe) (0.812)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from bpe) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpe) (4.41.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from bpe) (0.11.1)\n",
            "Requirement already satisfied: hypothesis in /usr/local/lib/python3.6/dist-packages (from bpe) (6.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (20.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (8.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->bpe) (53.0.0)\n",
            "Requirement already satisfied: mypy-extensions<0.5.0,>=0.4.3 in /usr/local/lib/python3.6/dist-packages (from mypy->bpe) (0.4.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy->bpe) (3.7.4.3)\n",
            "Requirement already satisfied: typed-ast<1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mypy->bpe) (1.4.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from hypothesis->bpe) (2.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRjptthOzy48"
      },
      "source": [
        "from torch import nn, tensor\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdDxF4IfFQgS"
      },
      "source": [
        "df = pd.read_csv(\"description_bisac.csv\", nrows=5000, names=[\"id\", \"mainDescription\", \"bisacCode\"])\n",
        "# TO-DO: Mask some words"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMcjLFiPXcZY"
      },
      "source": [
        "# Tokenizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEhuoI5SR6bk"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWqQCe53XbaS"
      },
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.train([\"./description_bisac.csv\"], trainer=trainer)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUDyIaATVM8b"
      },
      "source": [
        "*\"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqjmk0NFYN0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2b6b44-6db1-48d7-ca7c-4e74f9886717"
      },
      "source": [
        "o_alquimista = \"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"\n",
        "encoded = tokenizer.encode(o_alquimista)\n",
        "encoded.ids[:10]\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2800, 443, 1336, 122, 62, 140, 105, 1993, 11, 860]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZMcCBT0WH6V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lblI7YJ4cTkW"
      },
      "source": [
        "# Creating a Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB217vobcTGo"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbPSYTbdcb98"
      },
      "source": [
        "class MetabooksDataset(Dataset):\n",
        "    \"\"\"Metabooks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, tokenizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            tokenizer (huggingface Tokenizer): Will encode str sentences\n",
        "        \"\"\"\n",
        "        self.genres_df = pd.read_csv(\"description_bisac.csv\", names=[\"id\", \"mainDescription\", \"bisacCode\"])\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.tokenizer.encode(self.genres_df[\"mainDescription\"][idx])\n",
        "        encoded_size = len(encoded.ids)\n",
        "        # TO-DO: Encode as one hot\n",
        "        label = self.genres_df[\"bisacCode\"].astype(\"category\").cat.codes[idx]\n",
        "        return {\"text\": tensor(encoded.ids, dtype=torch.int64).view(1, encoded_size), \"label\": tensor(label, dtype=torch.int64)}"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDvUGpAjdKE-"
      },
      "source": [
        "mb_dataset = MetabooksDataset('description_bisac.csv', tokenizer)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hddd52ZgguS",
        "outputId": "52bac8a9-3b20-476b-a669-fe0c62cf69c2"
      },
      "source": [
        "mb_dataset[0]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': tensor(0),\n",
              " 'text': tensor([[ 607,  212,  103, 3895, 3954, 3983, 3993,  387,  140, 3967, 4008,   83,\n",
              "           371,   13, 2015, 2014,   54, 4015,    9, 2596, 1321, 3982,   95, 2540,\n",
              "          2547,    9, 2545, 1321, 2542,   55,  234, 2558,  141,   70,  745,  113,\n",
              "          1155, 1690, 3986,  103,  231, 4006, 3975,   13, 3965, 4005,  123,   97,\n",
              "           130, 2605,   60,  169, 3973,  134, 2630,   83, 3933, 1701,  103,  700,\n",
              "            70,  392,   56,  297,   11, 2389,  502,  567,   11,  274,  705,  117,\n",
              "          1341,  831,   11,  180, 2541,  106, 2546,  106, 3990,   11,  239,  552,\n",
              "           117,  433, 3935, 2656, 2015, 2014, 3979, 2019,   12,  134,  117,  633,\n",
              "           584, 2465, 3991,  113, 3949,   11, 3969,   12,  134,  117, 2304,   60,\n",
              "          3998,   13, 3994,  113, 3957,   31,   13, 3974, 3992, 2014,   11,  371,\n",
              "            13, 2015, 1722,  552, 2541,  106, 2546,  106, 2619, 3995,   13, 3899,\n",
              "          4013,  117, 4002, 1645,   11,  117, 2542,   11,  117,  140, 1701, 3956,\n",
              "           151, 2545,   11, 3913,   56, 3962,  151, 4009, 4010,  123, 3943,   56,\n",
              "           811,  103, 2540, 2547,   13,  479, 3964, 1722,   11,  130, 2446, 3958,\n",
              "            11, 4004,  140,  148, 2351, 2464,  103,  297,  101, 2416,   13, 1976,\n",
              "           419, 1643, 3963,   56,  268,  103, 3972,  103,  567,  151,  655,   60,\n",
              "           151,  423,   13]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpBtnyMiPU9"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU4LNPyNiSOZ"
      },
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKP_Z4C9pEZG",
        "outputId": "a120d69e-3b8e-454f-fd52-a8d0e295db16"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(20)):\n",
        "  data = mb_dataset[0]\n",
        "  outputs = model(data[\"text\"], labels=data[\"label\"])\n",
        "  loss = outputs.loss\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:03<00:00,  3.18s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sugfgyriVUxC",
        "outputId": "dc4707d3-1f95-4839-c00a-6ef9950169b9"
      },
      "source": [
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import BertForSequenceClassification  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OUVSCkJWS6y"
      },
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"pt\")"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm1u0un6phNt"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9nQSDKepk7w",
        "outputId": "432fd7bc-8a9f-42a4-8651-91f367b14dd0"
      },
      "source": [
        "o_alquimista = \"Quando você quer alguma coisa, todo o Universo conspira para que você realize seu desejo. De tempos em tempos, surge um livro que muda para sempre a vida de seus leitores. O Alquimista é um deles. Com mais de 65 milhões de exemplares vendidos no mundo inteiro, o mais famoso título de Paulo Coelho já se estabeleceu como um clássico moderno, atemporal e universal. Quase 25 anos após seu lançamento, segue fascinando públicos cada vez maiores, de diferentes gerações. Simples, sábia e inspiradora, esta história refaz os passos de um pastor da Andaluzia que viaja para o deserto egípcio em busca de um tesouro enterrado nas Pirâmides. O que começa como uma jornada para encontrar bens mate¬riais torna-se uma descoberta das riquezas que escondemos dentro de nós mesmos. As belas lições que Santiago aprende pelo caminho nos falam da sabedoria de ouvir o que diz nosso coração, ler os sinais com que deparamos ao longo da vida e, acima de tudo, seguir os nossos sonhos.\"\n",
        "encoded = tokenizer.encode(o_alquimista)\n",
        "ids = encoded.ids\n",
        "model(tensor(ids, dtype=torch.int64).view(1, len(ids)))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('logits',\n",
              "                           tensor([[-0.1214, -0.0963]], grad_fn=<AddmmBackward>))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    }
  ]
}
